---
title: "Fine-Tune Your LLM in MINUTES with Nebius ⚡️"
datePublished: Fri Mar 14 2025 13:21:17 GMT+0000 (Coordinated Universal Time)
cuid: cm88t3nch000009ju391pcnl1
slug: fine-tune-your-llm-in-minutes-with-nebius
canonical: https://dev.to/arindam_1729/fine-tune-your-llm-in-minutes-with-nebius-4e5h
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1741778328102/c6fb7646-7b32-4879-a43c-ce6a1e804c70.png
tags: ai, llm, finetuning

---

## Introduction

Large Language Models are Great! But it’s not that good when it comes to doing specific tasks or being a domain subject matter expert.

That’s where Fine tuning comes into the picture!

In this Article I’ll share how to fine-tune your LLMs in Minutes with Nebius Ai Studio.

So, without delaying further,

Let’s START!

## What is Fine Tuning?

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1740975483165/f467bea1-913b-4afb-9e61-71ab1a1373f8.png align="center")

Before we begin, let’s Understand What fine-tuning is!

In Simple Words, Fine-tuning is the process of taking a pre-trained model and further training it on a domain-specific dataset.

We all know how powerful the LLMs are nowadays, but they are far from perfect for specialized Tasks. And one of the best ways to make it such an expert is using Fine-Tuning with domain-specific knowledge.

This means re-training parts of the model on the exact tasks we want to achieve. This is less costly than retraining the whole model.

However, fine-tuning your LLM is not an easy task for folks like me who are not traditional ML engineers. Nebius AI Studio makes it easier to fine-tune your Open Source LLMs.

With their intuitive UI platform or Python SDKs, fine-tuning is accessible to everyone, whether they are into coding or not.

> 🎥 **Want a quick tutorial? Watch this step-by-step video:**
> 
> %[https://www.youtube.com/watch?v=07e6KzOYjnA] 

## How to FineTune your LLM with Nebius

Now, let’s see how you can easily fine-tune your LLMs with Nebius AI Studio.

**So, Nebius AI Studio** offers **three ways** to fine-tune your LLM:

* **Web Console** – A no-code solution for easy fine-tuning.
    
* **Python SDK** – A developer-friendly approach using Python.
    
* **cURL Requests** – API-based fine-tuning for automation and scripting.
    

We’ll see how to fine-tune your models with each approach.

### **Prerequisites**

#### **1\. Choose a Model**

At first, You have to choose the model you want to fine-tune. Nebius AI Studio supports fine-tuning for **30+ leading models**, including:

* **Llama 3 Series** (1B–70B parameters)
    
* **Qwen Series** (1.5B–72B parameters)
    
* **DeepSeek R1** and other specialized models
    

You can browse the [**Available Mod**](#)**els** and select the one that fits your use case.

#### **2\. Prepare Your Dataset**

Next, you have to prepare the datasets for training a model and validating the results of the training. Nebius AI Studio supports the [JSON](https://jsonlines.org/) Lines format (`. jsonl`) for dataset files.

> 💡 Tip : Split the data between two datasets as 80–90% for training and 10–20% for validation

#### **3\. Get Your API Key**

Get the Nebius [API key](https://studio.nebius.com/settings/api-keys) and store it in `.env` variables

```json
NEBIUS_API_KEY=<Your_API_key>
```

Once you have completed these steps, you’re ready to start fine-tuning your model.

### Fine-Tuning via Web Console (No Coding Required)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741761939046/3d317520-5027-497e-8e9b-3a53ad571687.png align="center")

The simplest way to fine-tune your LLM is through **Nebius AI Studio’s Web Console. You don’t need to write a single line of code for this.**

* Log in to **Nebius AI Studio** and go to the **Fine-tuning** section.
    
* Upload your training and validation datasets.
    
* Choose your model and fine-tuning parameters.
    
* Click **Create job**.
    
* Download files with the fine-tuned model using the **Checkpoints** link under the required model name.
    

I’ve created a video showing how to do that, you can check it [here](https://www.youtube.com/watch?v=07e6KzOYjnA).

### Fine-Tuning via Python SDK

If you prefer working with Python, you can fine-tune your model programmatically using Nebius’s Python SDK.

#### **1\. Install Dependencies**

First, we’ll install the Dependency with the following command:

```json
pip3 install openai
```

#### 2\. Set Up the OpenAI Client

Set up the OpenAI client with Nebius AI's API endpoint and your API key

```python
import os
from openai import OpenAI
import time

client = OpenAI(
    base_url="https://api.studio.nebius.com/v1/",
    api_key=os.environ.get("NEBIUS_API_KEY"),
)
```

#### 3\. Upload Your Datasets

Next, we’ll Upload our Training dataset and Validation Dataset that we’ve created previously

```python
# Upload a training dataset
training_dataset = client.files.create(
    file=open("<dataset_name>.jsonl", "rb"), # Specify the dataset name
    purpose="fine-tune"
)

# Upload a validation dataset
validation_dataset = client.files.create(
    file=open("<dataset_name>.jsonl", "rb"), # Specify the dataset name
    purpose="fine-tune"
)
```

#### 4\. Create a Fine-Tuning Job

After that, we need to define fine-tuning parameters, such as the **number of epochs** and whether to use **LoRA adaptation** for efficiency.

If you want to add more Parameters, check your [Nebius Docs for more](https://www.google.com/url?q=https%3A%2F%2Fdocs.nebius.com%2Fstudio%2Ffine-tuning%2Fhow-to-fine-tune%23instructions) information.

```json
job_request = {
    "model": "meta-llama/Llama-3.1-8B-Instruct", # Choose the model
    "training_file": training_dataset.id,
    "validation_file": validation_dataset.id,
    "hyperparameters": {
        "n_epochs": 3, # Number of epochs for training
        "lora": True,  # Enable LoRA for fine-tuning efficiency
    },
    "integrations": [],
}
```

We’ll then create and run the fine-tuning job

```python
 # Create and run the fine-tuning job
job = client.fine_tuning.jobs.create(**job_request)
```

#### 5\. Monitor the Job Status

If you are fine-tuning models like `meta-llama/Llama-3.1-70B-Instruct` might take some time. For that we’ll periodically check if the job is still running or has completed:

```python
# Make sure that the job has been finished or cancelled
active_statuses = ["validating_files", "queued", "running"]
while job.status in active_statuses:
    time.sleep(15)
    job = client.fine_tuning.jobs.retrieve(job.id)
    print("current status is", job.status)

print("Job ID:", job.id)
```

#### 6\. Retrieve Checkpoints and Save

Fine-tuning creates **multiple checkpoints** (intermediate versions of the model). We’ll retrieve them and save the files into these directories.

```python
if job.status == "succeeded":
    # Check the job events
    events = client.fine_tuning.jobs.list_events(job.id)
    print(events)

    for checkpoint in client.fine_tuning.jobs.checkpoints.list(job.id).data:
        print("Checkpoint ID:", checkpoint.id)

        # Create a directory for every checkpoint
        os.makedirs(checkpoint.id, exist_ok=True)

        for model_file_id in checkpoint.result_files:
            # Get the name of a model file
            filename = client.files.retrieve(model_file_id).filename

            # Retrieve the contents of the file
            file_content = client.files.content(model_file_id)

            # Save the contents into a local file
            file_content.write_to_file(filename)
```

And We’ve Fine-tuned our Model! Next up, we’ll host these models.

**You can try this method interactively on Google Colab:** [Colab Notebook](https://colab.research.google.com/drive/1rbl7lLEubENGOqZ3GWJ72iw8Jpnr727G?usp=sharing)

### Fine-Tuning via cURL Requests

If you prefer **API-based fine-tuning** or want to **automate** the process, you can use **cURL requests** to interact with Nebius AI Studio directly.

#### 1\. Upload Your Dataset

Run the following cURL command to upload your training dataset:

```json
curl 'https://api.studio.nebius.com/v1/files' \
   -H 'Accept: application/json' \
   -H 'Content-Type: multipart/form-data' \
   -H "Authorization: Bearer $NEBIUS_API_KEY"
   -F 'file=@<dataset-name>.jsonl' \
   -F 'purpose=fine-tune'
```

> 💡 Note : Save the file ID from the response ; it is required to create a fine-tuning job.

Similarly, upload your validation dataset:

```python
curl 'https://api.studio.nebius.com/v1/files' \
   -X 'POST' \
   -H 'Authorization: Bearer $NEBIUS_API_KEY' \
   -H 'Accept: application/json' \
   -H 'Content-Type: multipart/form-data' \
   -F 'file=@validation_dataset.jsonl' \
   -F 'purpose=fine-tune'
```

#### 2\. Send the Fine-Tuning Request:

Now, we’ll create a fine-tuning job by running the following command:

```json
curl 'https://api.studio.nebius.com/v1/fine_tuning/jobs' \
   -X 'POST' \
   -H 'Authorization: Bearer $NEBIUS_API_KEY' \
   -H 'Accept: application/json' \
   -H 'Content-Type: application/json' \
   -d '{
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "training_file": "your_training_file_id",
      "validation_file": "your_validation_file_id",
      "hyperparameters": {
         "n_epochs": 3, 
         "batch_size": 8,
         "learning_rate": 0.0001,
         "lora": true,
         "lora_r": 16,
         "lora_alpha": 32,
         "lora_dropout": 0.1,
         "weight_decay": 0.01
      }
   }'
```

> **💡 Replace the placeholders:**
> 
> * `"your_training_file_id"` → File ID from Step 2.
>     
> * `"your_validation_file_id"` → File ID from Step 2.
>     

After submission, Nebius AI Studio will return a job ID. Keep this for tracking progress

#### 4\. Monitor the Fine-Tuning Job

Fine-tuning takes time. To check the status of the job, Run this:

```json
curl 'https://api.studio.nebius.com/v1/fine_tuning/jobs/<job_ID>' \
   -X 'GET' \
   -H 'Accept: application/json' \
   -H 'Authorization: Bearer $NEBIUS_API_KEY'
```

Make sure that the training has been successful. To do this, check the job events. They are created when the job status changes.

```json
curl 'https://api.studio.nebius.com/v1/fine_tuning/jobs/<job_ID>/events' \
   -X 'GET' \
   -H 'Accept: application/json' \
   -H 'Authorization: Bearer $NEBIUS_API_KEY' \
   --url-query limit=<...> \
   --url-query after=<...>
```

#### 5\. Retrieve Checkpoints

Once fine-tuning is **complete**, you can retrieve the **trained model checkpoints**. These are intermediate versions of your model saved at different stages.

```json
curl 'https://api.studio.nebius.com/v1/fine_tuning/jobs/<job_ID>/checkpoints' \
   -X 'GET' \
   -H 'Accept: application/json' \
   -H 'Authorization: Bearer $NEBIUS_API_KEY'
```

We’ll get the checkpoint IDs in the response.

Each checkpoint consists of multiple files containing different parts of the fine-tuned model. To get the **file names and extensions**, use:

```python
curl 'https://api.studio.nebius.com/v1/files/<file_ID>' \
   -X 'GET' \
   -H 'Accept: application/json' \
   -H 'Authorization: Bearer $NEBIUS_API_KEY'
```

**Repeat this request for each file in the checkpoint to download all required files.**

#### 6\. Download the Fine-Tuned Model

Once you retrieve the model file contents, make sure to **save them with the correct filename and extension**

```json
curl 'https://api.studio.nebius.com/v1/files/<file_ID>/content' \
   -X 'GET' \
   -H 'Accept: application/json' \
   -H 'Authorization: Bearer $NEBIUS_API_KEY'
```

**Repeat this request for each file in the checkpoint to download all required files.**

#### 7\. Save Model Files Properly

Once you retrieve the model file contents, make sure to **save them with the correct filename and extension**

## **Hosting Your Fine-Tuned Model on Nebius AI Studio**

Once your model is fine-tuned, the next step is **making it accessible** for yourself and others. Hosting your model on **Nebius AI Studio** allows you to deploy and use it seamlessly.

Whether it's a **fine-tuned LLM** or a **LoRA adapter**, you can request hosting directly through the platform.

Before you start, ensure you have:  
✅ A **fine-tuned model** ready for deployment.  
✅ A **shared cloud storage link** containing the model files in an archive.

Follow these simple steps to **host your model** in Nebius AI Studio:

In the [web console](https://studio.nebius.com/), go to the **Fine-tuning** section.

1. Click  **Request model or LoRA hosting**
    
    ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741766583763/a9af614a-8b17-4899-a4e2-b79692b55516.png align="center")
    
2. In the window that opens, select **Request LoRA model hosting**.
    
    ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741766498392/ca9289bd-c8f1-4dd3-b173-89c14dc140f1.png align="center")
    
3. Enter the **model name** and ensure your **email** is correct.
    
4. Paste the **link to your model files archive** and (if applicable) describe your **LoRA adapter type**.
    
5. Click **Submit** and your request will be sent to the **Nebius AI support team** for processing.
    

Once approved, your model will be hosted and ready to use!

## Conclusion

That’s the Wrap! In this article, we learnt how easily we can fine-tune our LLM in 3 different approaches.

If you found this article useful, share it with your peers and community to spread the word about this.

Feel free to comment, DM me if you have any questions regarding this!

Also, Follow me For More Content like this.

For Paid collaboration mail me at: [arindammajumder2020@gmail.com](https://mailto:arindammajumder2020@gmail.com/).

Thank you for Reading!